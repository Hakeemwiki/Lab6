# Use official Spark with PySpark base image
FROM apache/spark-py

# Switch to root for installations
USER root

# Install basic tools and ensure python3 is available as 'python'
RUN apt-get update && apt-get install -y \
    curl \
    procps \
    && ln -s /usr/bin/python3 /usr/bin/python \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Add Delta Lake JAR (Spark 3.4.x compatible)
ADD https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.2/delta-spark_2.12-3.3.2.jar /opt/spark/jars/

# Add Hadoop AWS and AWS SDK JARs for S3 access
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar /opt/spark/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar /opt/spark/jars/

# Copy Spark transformation script
COPY src/transformation/transform.py .

# Run using spark-submit with Delta Lake config
CMD ["spark-submit","--conf", "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","--conf", "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","transform.py","s3a://lab6-ecommerce-shop/incoming/orders_*.csv","s3a://lab6-ecommerce-shop/incoming/order_items_*.csv","s3a://lab6-ecommerce-shop/incoming/products.csv"]
