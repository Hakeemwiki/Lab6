# Base Spark + Python image
FROM apache/spark-py

# Use root for installs
USER root

# Install required packages and create python symlink
RUN apt-get update && apt-get install -y \
    curl \
    procps \
    && ln -s /usr/bin/python3 /usr/bin/python \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Add required JARs for S3 support (based on Hadoop 3.3.2)
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar /opt/spark/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar /opt/spark/jars/

# Copy transformation script
COPY src/transformation/transform.py .

# Optional: Set env for Spark local
ENV SPARK_LOCAL_IP=127.0.0.1

# Default command (can be overridden by ECS)
CMD ["spark-submit", "--master", "local[*]", "transform.py"]
