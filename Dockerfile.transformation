# Use Bitnami Spark 3.3.2 with PySpark
FROM bitnami/spark:3.3.2

# Switch to root for installations
USER root

# Install system tools and Python alias
RUN apt-get update && apt-get install -y curl procps && ln -s /usr/bin/python3 /usr/bin/python

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Add Delta Lake and Hadoop AWS JARs (All Spark 3.3.x compatible)
ADD https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/2.4.0/delta-spark_2.12-2.4.0.jar /opt/bitnami/spark/jars/
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar /opt/bitnami/spark/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1026/aws-java-sdk-bundle-1.11.1026.jar /opt/bitnami/spark/jars/

# Copy transformation script
COPY src/transformation/transform.py .

# Run with Delta and S3 support
CMD ["spark-submit","--conf", "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","--conf", "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain","transform.py","s3a://lab6-ecommerce-shop/incoming/orders_*.csv","s3a://lab6-ecommerce-shop/incoming/order_items_*.csv","s3a://lab6-ecommerce-shop/incoming/products.csv"]
